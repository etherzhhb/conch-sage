
# Embedding Provider Design and Integration

## ğŸ“Œ Overview
This document captures the design discussion around integrating real embedding functionality into the Conch Sage research assistant. It covers three main areas:

1. Embedding backend **options**
2. Relationship between `LLMProvider` and `EmbeddingProvider`
3. Comparative analysis of embedding **providers** based on pricing, performance, and compatibility

---

## ğŸ” 1. Embedding Backend Options

The original system used a mock embedding vector (`[0.1] * 768`). To support real semantic search, we now plan to use a pluggable `EmbeddingProvider` interface.

### Options:

- **OpenAI Embeddings** (`text-embedding-3-small`, etc.)
- **AWS Bedrock** (Titan Text Embeddings)
- **Claude/Anthropic** (no dedicated embedding API yet)
- **Sentence-Transformers** (local, Hugging Face models)
- **Cohere Embed API** (cloud-based)
- **Hugging Face Inference API**
- **Ollama** (âš ï¸ currently does not support embeddings)

---

## ğŸ§© 2. LLMProvider vs. EmbeddingProvider

### ğŸ§  Conceptual Separation
| Component          | Purpose                                | Example Backends                   |
|-------------------|----------------------------------------|------------------------------------|
| `LLMProvider`     | Generates completions, chat, summaries | `OpenAI`, `Claude`, `Ollama`       |
| `EmbeddingProvider` | Converts text to dense vectors          | `OpenAI`, `Titan`, `MiniLM`        |

### âœ… Recommended Design: Keep Them Separate
- Clean separation of concerns
- Each has its own configuration section
- `ConversationGraph` exposes:

```python
get_llm() -> LLMProvider
get_embedding_provider() -> EmbeddingProvider
```

### Alternatives Considered
- **Embedding as method of LLMProvider**: breaks abstraction, not all LLMs embed
- **Unified Provider**: over-complicated, tight coupling

---

## ğŸ“Š 3. Embedding Provider Comparison

| Provider                   | Quality | Latency | Cost                     | Offline | Notes                          |
|---------------------------|---------|---------|--------------------------|---------|---------------------------------|
| OpenAI (`text-embedding`) | ğŸ”¥ High | âš¡ Fast  | $0.0001â€“0.00013 / 1K tokens | âŒ      | Best accuracy, cloud only     |
| AWS Titan (Bedrock)       | âœ… Good | âš ï¸ Medium | Bedrock pricing          | âŒ      | AWS-focused workflows         |
| Claude (Bedrock)          | ğŸŸ¡ Fair | âš ï¸ Medium | N/A                      | âŒ      | No public embedding API       |
| Sentence-Transformers     | âœ… Good | âš¡ Fast  | Free (local)             | âœ…      | Best for offline use          |
| Cohere Embed              | âœ… Good | âš¡ Fast  | Free tier + paid         | âŒ      | Cloud-based alternative       |
| HF Inference API          | âœ… Good | âš ï¸ Medium | Free tier + paid         | âŒ      | Easy to swap models           |
| Ollama                    | âŒ None | â€”       | â€”                        | âœ…      | No embedding support          |

### Compatibility Notes

| LLM Provider | Suggested Embedding Backend |
|--------------|-----------------------------|
| OpenAI       | OpenAI                      |
| Ollama       | Sentence-Transformers       |
| Claude       | Titan, or external          |
| Mock         | MockEmbeddingProvider       |

---

## ğŸ§­ Recommendation by Use Case

### âœ… If you want the best quality and already use OpenAI
Use `text-embedding-3-small` or `text-embedding-ada-002`.

```yaml
embedding:
  provider: openai
  model: text-embedding-3-small
```

### âœ… If you need full offline/local capability
Use `sentence-transformers`, e.g., `all-MiniLM-L6-v2` or `bge-small-en`.

```yaml
embedding:
  provider: sentence-transformers
  model: all-MiniLM-L6-v2
```

### âœ… If you're deeply integrated with AWS
Use **Titan Embeddings** via Bedrock (if supported in your region/account).

```yaml
embedding:
  provider: bedrock
  model: amazon.titan-embed-text-v1
```

### ğŸ§ª Experimental Setup
Support per-node or per-session embedding backend overrides to compare results side-by-side.

---

## âœ… Implementation Plan

Start with `sentence-transformers` for local support. Then optionally add OpenAI and Bedrock.

```yaml
embedding:
  provider: sentence-transformers
  model: all-MiniLM-L6-v2
```
